# -*- coding: utf-8 -*-
"""DLdropout.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uXjb646iQrHC82SWcFOUFGkkVdDMRS0x
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x
import tensorflow as tf
from tensorflow.keras import utils,layers,datasets,models
import numpy as np
import matplotlib.pyplot as plt

train_size=49000
batch_size=100
learning_rate= 10**(-5)
epochs=60

def main():

  tf.reset_default_graph()
  tf.set_random_seed(seed=0)

  (x_trainval, y_trainval), (x_test, y_test) = datasets.cifar10.load_data()
  indices = np.random.permutation(len(x_trainval))

  train_indices = indices[0:train_size]
  validation_indices = indices[train_size:]

  x_trainval = x_trainval/255
  x_test = x_test/255
  y_trainval = utils.to_categorical(y_trainval,num_classes=10)
  y_test = utils.to_categorical(y_test,num_classes=10)

  X = tf.placeholder(tf.float32, (None, 32,32,3))
  Y = tf.placeholder(tf.float32, (None, 10))
  K = tf.placeholder(tf.float32, name='K')

  W_conv1 = tf.Variable(tf.truncated_normal([3,3,3,32], stddev=0.1)) #32 filter
  b_conv1 = tf.Variable(tf.zeros(shape=(32,)))
  A_conv1 = tf.nn.relu(tf.nn.conv2d(X, W_conv1, strides=[1,1,1,1], padding='SAME') + b_conv1) # ? x 32 x 32 x 32

  W_conv2 = tf.Variable(tf.truncated_normal([3,3,32,32], stddev=0.1)) #32 filter
  b_conv2 = tf.Variable(tf.zeros(shape=(32,)))
  A_conv2 = tf.nn.relu(tf.nn.conv2d(A_conv1, W_conv2, strides=[1,1,1,1], padding='SAME') + b_conv2) # ? x 32 x 32 x 32

  A_pool1 = tf.nn.max_pool(A_conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME') # ? x 16x16x32

  D_out1 = tf.nn.dropout( A_pool1, K)

  W_conv3 = tf.Variable(tf.truncated_normal([3,3,32,64], stddev=0.1)) #64 filter
  b_conv3 = tf.Variable(tf.zeros(shape=(64,)))
  A_conv3 = tf.nn.relu(tf.nn.conv2d(D_out1, W_conv3, strides=[1,1,1,1], padding='SAME') + b_conv3) # ? x 16 x 16 x 64

  W_conv4 = tf.Variable(tf.truncated_normal([3,3,64,64], stddev=0.1)) #64 filter
  b_conv4 = tf.Variable(tf.zeros(shape=(64,)))
  A_conv4 = tf.nn.relu(tf.nn.conv2d(A_conv3, W_conv4, strides=[1,1,1,1], padding='SAME') + b_conv4) # ? x 16 x 16 x 64

  A_pool2 = tf.nn.max_pool(A_conv4, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME') # ? x 8x8x64

  D_out2 = tf.nn.dropout(A_pool2, K)

  A_pool2_flat = tf.reshape(D_out2, [-1,8*8*64]) # ? x 4096

  W_fc1 = tf.Variable(tf.truncated_normal([8*8*64, 512], stddev=0.1))
  b_fc1 = tf.Variable(tf.zeros(shape=(512,)))

  A_fc1 = tf.nn.relu(tf.matmul(A_pool2_flat, W_fc1) + b_fc1) # ? x 512

  D_out3 = tf.nn.dropout(A_fc1, K)

  W_fc2 = tf.Variable(tf.truncated_normal([512, 10], stddev=0.1))
  b_fc2 = tf.Variable(tf.zeros(shape=(10,)))

  Z = tf.matmul(D_out3, W_fc2) + b_fc2

  loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=Z)
  loss = tf.reduce_mean(loss)

  hits = tf.equal(tf.argmax(Z, axis=1), tf.argmax(Y, axis=1))
  accuracy = tf.reduce_mean(tf.cast(hits, tf.float32))

  optimizer = tf.train.AdamOptimizer(learning_rate)
  train = optimizer.minimize(loss)

  session = tf.Session()
  session.run(tf.global_variables_initializer())
  n_batches = train_size // batch_size
  plotTrainLoss=[]
  plotValLoss=[]
  plotEpoch=[]
  plotAccuracy=[]
  plotTrainAccuracy=[]
  for epoch in range(epochs):
    np.random.shuffle(train_indices)

    k=0
    train_loss=0
    train_accu=0
    for t in range(0, 49000, batch_size):
      trainloss, trainaccu, _ = session.run( [loss, accuracy, train], {X: x_trainval[train_indices[t:t+batch_size]], Y: y_trainval[train_indices[t:t+batch_size]],K: 0.5})
      train_loss+=trainloss
      train_accu+=trainaccu
      k+=1
    val_loss, val_accu = session.run([loss, accuracy], {X: x_trainval[validation_indices], Y: y_trainval[validation_indices],K: 1.0})
    plotValLoss.append(val_loss)
    plotAccuracy.append(val_accu)
    plotEpoch.append(epoch)
    plotTrainLoss.append(train_loss/k)
    plotTrainAccuracy.append(train_accu/k)
    print('Epoch: {0}. Training loss: {1}. Training Accuracy: {2}. Val loss:{3}. Val accu:{4}'.format(epoch,train_loss/k, train_accu/k,val_loss,val_accu))
    _,test_accu = session.run([loss, accuracy], {X: x_test, Y: y_test,K: 1.0})
  print('Test accuracy: {0} %'.format(test_accu*100))
  session.close()

  plt.plot(plotTrainLoss,label="Training loss")
  plt.plot(plotValLoss, label="Validation loss")
  plt.ylabel("Loss")
  plt.xlabel("Epoch")
  plt.legend()
  plt.show()
  plt.plot(plotAccuracy, label="Validation accuracy")
  plt.plot(plotTrainAccuracy, label="Train accuracy")
  plt.ylabel("Accuracy")
  plt.xlabel("Epoch")
  plt.legend()
  plt.show()

main()
